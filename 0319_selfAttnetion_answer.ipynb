{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782f2f38",
   "metadata": {},
   "source": [
    "RNN 기반의 LM과 Transformer 계열의 LM의 가장 큰 차이점은?\t\t\t\n",
    "\n",
    "\t아키텍처 구조\t\t\t\n",
    "\t\t순환 신경망 vs 인코더 디코더 구조(+셀프어텐선 피드포워드 레이어 등으로 구성)\t\t\n",
    "\t\t\t\t\n",
    "\t학습속도와 병렬처리\t\t\t\n",
    "\t\tattention vs self attention\t\t\n",
    "\t\t순환 처리 vs 병렬 처리\t\t\n",
    "\t장기 의존성 모델링\t\t\t\n",
    "\t\t긴 시퀀스 처리 정도\t\t\n",
    "\t메모리 요구량\t\t\t\n",
    "\t\t과거 정보 vs self attention \t\t\n",
    "\t결국 self attention이 가장 큰 차이로 보임\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t이전단어 + 시퀀스 내 주어진 단어\t\t\n",
    "\t\t시퀀스 내 주어진 모든 단어\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\tself attention이란?\t\t\t\n",
    "\t\t\b\t\t\n",
    "\tattention과 self attention의 차이\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e103b7",
   "metadata": {},
   "source": [
    "### attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5ef6ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention 출력: [[0.41954816 0.37053324 0.50633831 0.37451849]]\n",
      "Attention 가중치: [[0.4257442  0.35472567 0.21953012]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def attention(query, keys, values):\n",
    "    # 쿼리와 키의 내적\n",
    "    scores = np.dot(query, keys.T)\n",
    "\n",
    "    \n",
    "    # 소프트맥스 함수를 사용하여 어텐션 가중치 계산\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # 가중 평균 계산\n",
    "    outputs = np.dot(attention_weights, values)\n",
    "    \n",
    "    return outputs, attention_weights #아웃풋과 가중 평균 리턴\n",
    "\n",
    "def softmax(x):\n",
    "    ex_x = np.exp(x)\n",
    "    result = ex_x/np.sum(ex_x, axis=-1, keepdims=True)\n",
    "    #소프트 맥스 함수 구현\n",
    " \n",
    "    return result\n",
    "\n",
    "# 입력 시퀀스 및 쿼리 생성\n",
    "inputs = np.random.rand(3, 4)\n",
    "# numpy array 3*3 생성 랜덤 생성 0~1값\n",
    "query = np.random.rand(1, 4)\n",
    "# numpy array 1*3 생성 랜덤 생성 0~1값\n",
    "\n",
    "# 키와 값은 입력과 동일하게 설정\n",
    "keys = inputs\n",
    "values = inputs\n",
    "\n",
    "# Attention 함수 호출\n",
    "attention_output, attention_weights = attention(query, keys, values)\n",
    "\n",
    "print(\"Attention 출력:\", attention_output)\n",
    "print(\"Attention 가중치:\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb26057",
   "metadata": {},
   "source": [
    "### self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5193bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention 출력: [[1.05025364 1.00416134 0.63800809]\n",
      " [1.05120222 1.00857821 0.63790585]\n",
      " [1.03347266 0.98628393 0.62897492]]\n",
      "Self-Attention 가중치: [[0.39499636 0.35837366 0.24662998]\n",
      " [0.37463132 0.38657976 0.23878892]\n",
      " [0.37104345 0.34365691 0.28529964]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def self_attention(inputs):\n",
    "    # 입력 행렬의 크기\n",
    "    seq_len, embed_dim = inputs.shape\n",
    "    \n",
    "    # 쿼리, 키, 값 행렬 생성\n",
    "    queries = inputs\n",
    "    keys = inputs\n",
    "    values = inputs\n",
    "    \n",
    "    # 쿼리와 키의 내적\n",
    "    scores = np.dot(queries, keys.T)\n",
    "\n",
    "    \n",
    "    # 소프트맥스 함수를 사용하여 어텐션 가중치 계산\n",
    "    attention_weights = softmax(scores)\n",
    "\n",
    "    # 가중 평균 계산\n",
    "    outputs = np.dot(attention_weights, scores)\n",
    "    \n",
    "    return outputs, attention_weights\n",
    "\n",
    "def softmax(x):\n",
    "    #소프트 맥스 함수 구현\n",
    "    exp_x = np.exp(x)\n",
    "    result = exp_x/np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    return result\n",
    "\n",
    "# 입력 시퀀스 생성\n",
    "inputs = np.random.rand(3,3)# numpy array 3*3 생성 랜덤 생성 0~1값\n",
    "\n",
    "# Self-Attention 함수 호출\n",
    "self_attention_output, self_attention_weights = self_attention(inputs)\n",
    "\n",
    "print(\"Self-Attention 출력:\", self_attention_output)\n",
    "print(\"Self-Attention 가중치:\", self_attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2346e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
