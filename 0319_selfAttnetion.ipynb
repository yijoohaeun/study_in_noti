{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8afad60",
   "metadata": {},
   "source": [
    "### attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71a9c036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91125465, 1.12241751, 0.41889042])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.rand(3)\n",
    "np.dot(query, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "100963cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention 출력: [0.03047172 0.33645069 0.0602186 ]\n",
      "Attention 가중치: [0.17456152 0.5800437  0.24539478]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def attention(query, keys, values):\n",
    "    # 쿼리와 키의 내적\n",
    "    scores = np.dot(query, keys)\n",
    "    \n",
    "    # 소프트맥스 함수를 사용하여 어텐션 가중치 계산\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # 가중 평균 계산\n",
    "    attention_weightss = attention_weights/sum(attention_weights)\n",
    "    outputs = attention_weights*attention_weightss\n",
    "    outputs = np.dot()\n",
    "    \n",
    "    return outputs, attention_weights #아웃풋과 가중 평균 리턴\n",
    "\n",
    "def softmax(x):\n",
    "    su = sum(np.exp(x))\n",
    "    result = np.exp(x)/su\n",
    "    return result\n",
    "\n",
    "# 입력 시퀀스 및 쿼리 생성\n",
    "inputs = np.random.rand(3, 3)# numpy array 3*3 생성 랜덤 생성 0~1값\n",
    "query = np.random.rand(3)# numpy array 1*3 생성 랜덤 생성 0~1값\n",
    "\n",
    "# 키와 값은 입력과 동일하게 설정 # 모든 단어들, 주어진 단어.\n",
    "keys = inputs\n",
    "values = inputs.reshape(-1)\n",
    "\n",
    "# Attention 함수 호출\n",
    "attention_output, attention_weights = attention(query, keys, values)\n",
    "\n",
    "print(\"Attention 출력:\", attention_output)\n",
    "print(\"Attention 가중치:\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba2370",
   "metadata": {},
   "source": [
    "### self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee27451",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3701590807.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    inputs = # numpy array 3*3 생성 랜덤 생성 0~1값\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def self_attention(inputs):\n",
    "    # 입력 행렬의 크기\n",
    "    seq_len, embed_dim = inputs.shape  # btc\n",
    "    \n",
    "    # 쿼리, 키, 값 행렬 생성\n",
    "    queries = ''\n",
    "    keys = ''\n",
    "    values = ''\n",
    "    \n",
    "    # 쿼리와 키의 내적\n",
    "    scores = np.dot()\n",
    "\n",
    "    \n",
    "    # 소프트맥스 함수를 사용하여 어텐션 가중치 계산\n",
    "    attention_weights = softmax()\n",
    "\n",
    "    # 가중 평균 계산\n",
    "    outputs = np.dot()\n",
    "    \n",
    "    return outputs, attention_weights\n",
    "\n",
    "def softmax(x):\n",
    "    #소프트 맥스 함수 구현\n",
    "    return result\n",
    "\n",
    "# 입력 시퀀스 생성\n",
    "inputs = np.random.rand(3,3)# numpy array 3*3 생성 랜덤 생성 0~1값\n",
    "\n",
    "# Self-Attention 함수 호출\n",
    "self_attention_output, self_attention_weights = self_attention(inputs)\n",
    "\n",
    "print(\"Self-Attention 출력:\", self_attention_output)\n",
    "print(\"Self-Attention 가중치:\", self_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7632741",
   "metadata": {},
   "source": [
    "gpt, bert, transformer 써봤다면 꼭 알아야 함!!\n",
    "\n",
    "self-attention: 과거 정보를 활용해서 seq 내의 token의 유사도를 학습. \n",
    "\n",
    "multi-head attention..\n",
    "장기 기억 소실 gate - rnn의 단점을 해결하려고 나옴. \n",
    "이전 정보가 합쳐질 때 앞에 의 정보는 점점 소실됨. \n",
    "lstm, gru는 앞 단어와 다음단어 activate 함수를 통해 결정.\n",
    "다음단어 상관없이 이전단어가 합쳐지지 않고 ac 함수를 거친다. \n",
    "\n",
    "시퀀스로 집어넣을 때 앞 단어들에 댛나 정보가 한꺼번에 들어가서 소실이 x.\n",
    "memory 측면에서 transformer 가 더 좋음. rnn은 모든 단어를 가지고 있어야 함. llm은 딱 seq만 들어감.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
